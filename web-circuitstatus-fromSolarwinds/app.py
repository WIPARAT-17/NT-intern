import requests
import re
import html
import json
import xml.etree.ElementTree as ET
from ftfy import fix_text
import io
import csv
import datetime
import os
import argparse
import pandas as pd
from flask import Flask, request, render_template, jsonify, send_from_directory
import tempfile
import threading
import uuid
from reportlab.lib.pagesizes import letter, landscape # ‡πÄ‡∏û‡∏¥‡πà‡∏° landscape ‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤
from reportlab.platypus import SimpleDocTemplate, Table, Paragraph, Spacer, PageBreak
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.lib.units import inch
from reportlab.pdfbase import pdfmetrics
from reportlab.pdfbase.ttfonts import TTFont
import logging
from queue import Queue
import zipfile
import shutil
import time

app = Flask(__name__)

# --- ‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏•‡∏∞ Lock ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Thread-safe ---
processing_status = {}
status_lock = threading.Lock()

# --- ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Logger ‡πÅ‡∏•‡∏∞ Log Queue ---
log_queue = Queue()

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

if logger.handlers:
    for handler in list(logger.handlers):
        logger.removeHandler(handler)

class QueueHandler(logging.Handler):
    def __init__(self, queue):
        super().__init__()
        self.queue = queue

    def emit(self, record):
        try:
            msg = self.format(record)
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡∏Å‡∏£‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏≠‡∏≠‡∏Å
            if "üìÅ ‡∏•‡∏ö‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå CSV/PDF ‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß" in msg:
                return # ‡πÑ‡∏°‡πà‡πÉ‡∏™‡πà‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡∏µ‡πâ‡∏•‡∏á‡πÉ‡∏ô‡∏Ñ‡∏¥‡∏ß

            log_pattern = re.compile(r"^\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2},\d{3} - (INFO|WARNING|ERROR|CRITICAL) - (Job [0-9a-f-]+: )?(.*)")
            match = log_pattern.match(msg)
            if match:
                clean_msg = match.group(3)
                self.queue.put(clean_msg)
            else:
                self.queue.put(msg)

        except Exception:
            self.handleError(record)

console_handler = logging.StreamHandler()
console_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))
logger.addHandler(console_handler)

queue_handler = QueueHandler(log_queue)
logger.addHandler(queue_handler)

# --- ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏ü‡∏≠‡∏ô‡∏ï‡πå‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö PDF ---
THAI_FONT_NAME = 'THSarabunNew'
THAI_FONT_PATH = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'THSarabunNew.ttf')

THAI_FONT_REGISTERED = False
if os.path.exists(THAI_FONT_PATH):
    try:
        pdfmetrics.registerFont(TTFont(THAI_FONT_NAME, THAI_FONT_PATH))
        THAI_FONT_REGISTERED = True
        logger.info(f"Thai font '{THAI_FONT_NAME}' registered successfully from '{THAI_FONT_PATH}'.")
    except Exception as e:
        logger.error(f"ERROR: Could not register Thai font '{THAI_FONT_NAME}'. Error: {e}")
else:
    logger.warning(f"WARNING: Thai font file '{THAI_FONT_PATH}' not found. Please ensure the font file is in the same directory as the script.")

# --- ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ---
def get_data_from_api(nod_id, itf_id, job_id):
    """‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å API ‡πÅ‡∏•‡∏∞‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô JSON"""
    url = "http://1.179.233.116:8082/api_csoc_02/server_solarwinds_gin.php" 
    headers = {
        # ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô IP Address ‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ‡πÉ‡∏ô SOAPAction ‡∏î‡πâ‡∏ß‡∏¢
        "SOAPAction": "http://1.179.233.116/api_csoc_02/server_solarwinds_gin.php/circuitStatus",
        "Content-Type": "text/xml; charset=utf-8",
    }
    body = f"""<?xml version="1.0" encoding="utf-8"?>
<soap:Envelope xmlns:soap="http://schemas.xmlsoap.org/soap/envelope/"
               xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
               xmlns:xsd="http://www.w3.org/2001/XMLSchema">
  <soap:Body>
    <circuitStatus xmlns="http://1.179.233.116/soap/#Service_Solarwinds_gin"> <nodID>{nod_id}</nodID>
      <itfID>{itf_id}</itfID>
    </circuitStatus>
  </soap:Body>
</soap:Envelope>"""

    try:
        resp = requests.post(url, data=body, headers=headers, timeout=10)
        resp.raise_for_status()
        match = re.search(r"(<\?xml.*?</SOAP-ENV:Envelope>)", resp.text, re.DOTALL)
        if not match:
            logger.warning(f"‡πÑ‡∏°‡πà‡∏û‡∏ö XML Response ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö NodeID: {nod_id}, Interface ID: {itf_id}")
            return None
        
        root = ET.fromstring(match.group(1))
        return_tag = root.find(".//{*}return")
        if return_tag is None or not return_tag.text:
            logger.warning(f"API ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö NodeID: {nod_id}, Interface ID: {itf_id}")
            return None

        raw_text = return_tag.text
        html_unescaped = html.unescape(raw_text)
        fixed_text = fix_text(bytes(html_unescaped, "utf-8").decode("unicode_escape"))
        parsed_json = json.loads(fixed_text)
        return parsed_json
    except requests.exceptions.RequestException as req_e:
        logger.error(f"‚ùå ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• NodeID: {nod_id}, Interface ID: {itf_id} ‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß: {req_e}")
        return None
    except ET.ParseError as parse_e:
        logger.error(f"‚ùå XML Parsing ‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö NodeID: {nod_id}, Interface ID: {itf_id}: {parse_e}")
        return None
    except json.JSONDecodeError as json_e:
        logger.error(f"‚ùå JSON Decoding ‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö NodeID: {nod_id}, Interface ID: {itf_id}: {json_e}")
        return None
    except Exception as e:
        logger.error(f"‚ùå ‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÑ‡∏°‡πà‡∏Ñ‡∏≤‡∏î‡∏Ñ‡∏¥‡∏î‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö NodeID: {nod_id}, Interface ID: {itf_id}: {e}")
        return None

def process_json_data(raw_json_data, job_id, excel_node_id, excel_agency_name):
    """
    ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• JSON ‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å API ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå CSV/PDF
    - ‡πÄ‡∏ï‡∏¥‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏ö 24 ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ß‡∏±‡∏ô
    - ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏£‡∏ß‡∏° (Grand Total Average)
    - ‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö Bandwidth ‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏¥‡∏°‡∏≤‡∏ì‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô

    Parameters:
    - raw_json_data (list or dict): ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• JSON ‡∏î‡∏¥‡∏ö‡∏à‡∏≤‡∏Å API
    - job_id (str): ID ‡∏Ç‡∏≠‡∏á‡∏á‡∏≤‡∏ô‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö logging
    - excel_node_id (str): Node ID ‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå Excel (‡πÉ‡∏ä‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏´‡∏≤‡∏Å API ‡πÑ‡∏°‡πà‡∏°‡∏µ)
    - excel_agency_name (str): ‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå Excel (‡πÉ‡∏ä‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏´‡∏≤‡∏Å API ‡πÑ‡∏°‡πà‡∏°‡∏µ)

    Returns:
    - tuple: (headers, processed_data, grand_total_row)
        - headers (list): ‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢
        - processed_data (list): ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏•‡πâ‡∏ß‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏≤‡∏£‡∏≤‡∏á
        - grand_total_row (dict): ‡πÅ‡∏ñ‡∏ß Grand Total (Average)
    """
    column_mapping = {
        "‡∏£‡∏´‡∏±‡∏™‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô": "Customer_Curcuit_ID",
        "‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô": "Address",
        "‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏•‡∏∞‡πÄ‡∏ß‡∏•‡∏≤": "Timestamp",
        "‡∏Ç‡∏ô‡∏≤‡∏îBandwidth (‡∏´‡∏ô‡πà‡∏ß‡∏¢ Mbps)": "Bandwidth",
        "In_Averagebps": "In_Averagebps",
        "Out_Averagebps": "Out_Averagebps"
    }
    desired_headers_th = list(column_mapping.keys())
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏õ‡πá‡∏ô list ‡∏´‡∏£‡∏∑‡∏≠ dict ‡πÅ‡∏•‡πâ‡∏ß‡πÅ‡∏õ‡∏•‡∏á‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô list ‡πÄ‡∏™‡∏°‡∏≠
    data_to_process = raw_json_data if isinstance(raw_json_data, list) else [raw_json_data]

    if not data_to_process:
        logger.warning(f"Job {job_id}: ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• JSON ‡πÉ‡∏´‡πâ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•")
        return desired_headers_th, [], {}

    # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏±‡∏Å (‡∏£‡∏´‡∏±‡∏™‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô, ‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô, Bandwidth)
    # ‡∏Ñ‡πà‡∏≤‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡πÉ‡∏ä‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤‡∏ï‡∏±‡πâ‡∏á‡∏ï‡πâ‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏´‡∏±‡∏ß‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÉ‡∏ô PDF ‡πÅ‡∏•‡∏∞‡∏ñ‡∏π‡∏Å‡πÄ‡∏ï‡∏¥‡∏°‡∏•‡∏á‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÅ‡∏ñ‡∏ß‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
    # ‡∏´‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å API ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏µ‡πâ
    #customer_circuit_id_to_use = excel_node_id # ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡πÉ‡∏ä‡πâ‡∏à‡∏≤‡∏Å Excel
    address_to_use = excel_agency_name # ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡πÉ‡∏ä‡πâ‡∏à‡∏≤‡∏Å Excel
    bandwidth_to_use = '' # ‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤ ‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏à‡∏≤‡∏Å API ‡∏´‡∏£‡∏∑‡∏≠‡πÉ‡∏ä‡πâ raw

    first_item = data_to_process[0]

    # ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å API ‡∏Å‡πà‡∏≠‡∏ô ‡∏´‡∏≤‡∏Å‡∏°‡∏µ‡∏Ñ‡πà‡∏≤ ‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å API ‡πÅ‡∏ó‡∏ô‡∏Ñ‡πà‡∏≤‡∏à‡∏≤‡∏Å Excel
    api_customer_circuit_id = first_item.get(column_mapping["‡∏£‡∏´‡∏±‡∏™‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô"], '')
    api_address = first_item.get(column_mapping["‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô"], '')
    bandwidth_raw_from_api = first_item.get(column_mapping["‡∏Ç‡∏ô‡∏≤‡∏îBandwidth (‡∏´‡∏ô‡πà‡∏ß‡∏¢ Mbps)"], '')
    
    #if api_customer_circuit_id:
    #    customer_circuit_id_to_use = api_customer_circuit_id
    if api_address:
        address_to_use = api_address
    
    # ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• Bandwidth ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ (‡πÄ‡∏ä‡πà‡∏ô "20 Mbps.")
    if "FTTx" in str(bandwidth_raw_from_api):
        bandwidth_to_use = "20 Mbps."
    else:
        try:
            numeric_value_match = re.search(r'[\d.]+', str(bandwidth_raw_from_api))
            if numeric_value_match:
                numeric_value = float(numeric_value_match.group())
                bandwidth_to_use = f"{int(numeric_value)} Mbps."
            else:
                bandwidth_to_use = str(bandwidth_raw_from_api) # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏Å‡πá‡πÉ‡∏ä‡πâ raw string
        except (ValueError, TypeError, AttributeError):
            bandwidth_to_use = str(bandwidth_raw_from_api) # ‡∏ñ‡πâ‡∏≤‡πÅ‡∏õ‡∏•‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏Å‡πá‡πÉ‡∏ä‡πâ raw string


    # 1. ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤‡∏Ç‡∏≠‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô (‡πÄ‡∏ï‡πá‡∏°‡πÄ‡∏î‡∏∑‡∏≠‡∏ô)
    min_date_from_api = None
    max_date_from_api = None

    for item in data_to_process:
        if isinstance(item.get('Timestamp'), dict) and 'date' in item['Timestamp']:
            try:
                dt_obj_from_api = datetime.datetime.strptime(item['Timestamp']['date'], '%Y-%m-%d %H:%M:%S.%f')
                if min_date_from_api is None or dt_obj_from_api.date() < min_date_from_api:
                    min_date_from_api = dt_obj_from_api.date()
                if max_date_from_api is None or dt_obj_from_api.date() > max_date_from_api:
                    max_date_from_api = dt_obj_from_api.date()
            except ValueError:
                logger.warning(f"Job {job_id}: ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÅ‡∏õ‡∏•‡∏á‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏≤‡∏Å Timestamp ‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• API ‡πÑ‡∏î‡πâ: {item.get('Timestamp')}")
                continue

    # ‡∏´‡∏≤‡∏Å‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏à‡∏≤‡∏Å API ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô
    if min_date_from_api is None or max_date_from_api is None:
        today = datetime.date.today()
        report_start_date = today.replace(day=1)
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏ß‡∏±‡∏ô‡∏™‡∏¥‡πâ‡∏ô‡∏™‡∏∏‡∏î‡∏Ç‡∏≠‡∏á‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô
        next_month_start = (today.replace(day=1) + datetime.timedelta(days=32)).replace(day=1)
        report_end_date = next_month_start - datetime.timedelta(days=1)
        logger.warning(f"Job {job_id}: ‡πÑ‡∏°‡πà‡∏û‡∏ö Timestamp ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• API, ‡πÉ‡∏ä‡πâ‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏ß‡∏±‡∏ô‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á: {report_start_date} ‡∏ñ‡∏∂‡∏á {report_end_date}.")
    else:
        # ‡∏Ç‡∏¢‡∏≤‡∏¢‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏ó‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏ó‡∏µ‡πà‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• API ‡∏≠‡∏¢‡∏π‡πà
        report_start_date = min_date_from_api.replace(day=1)
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏ß‡∏±‡∏ô‡∏™‡∏¥‡πâ‡∏ô‡∏™‡∏∏‡∏î‡∏Ç‡∏≠‡∏á‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        next_month_start_for_max = (max_date_from_api.replace(day=1) + datetime.timedelta(days=32)).replace(day=1)
        report_end_date = next_month_start_for_max - datetime.timedelta(days=1)
        logger.info(f"Job {job_id}: ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ä‡πà‡∏ß‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• API: {report_start_date} ‡∏ñ‡∏∂‡∏á {report_end_date}.")

    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏∏‡∏Å‡∏ß‡∏±‡∏ô‡πÅ‡∏•‡∏∞‡∏ó‡∏∏‡∏Å‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î
    full_data_structure = {}
    current_date = report_start_date
    while current_date <= report_end_date:
        for hour in range(24):
            dt_obj = datetime.datetime.combine(current_date, datetime.time(hour, 0, 0))
            formatted_date_time = dt_obj.strftime('%Y-%m-%d %H.%M.%S')
            
            date_key = current_date.strftime('%Y-%m-%d')
            if date_key not in full_data_structure:
                full_data_structure[date_key] = {}

            # ‡πÄ‡∏ï‡∏¥‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• 24 ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤ customer_circuit_id_to_use, address_to_use, bandwidth_to_use
            # ‡πÅ‡∏•‡∏∞‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏Ç‡∏≠‡∏á‡∏õ‡∏£‡∏¥‡∏°‡∏≤‡∏ì‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÄ‡∏õ‡πá‡∏ô "0"
            full_data_structure[date_key][formatted_date_time] = {
                "‡∏£‡∏´‡∏±‡∏™‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô": api_customer_circuit_id, 
                "‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô": address_to_use,             
                "‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏•‡∏∞‡πÄ‡∏ß‡∏•‡∏≤": formatted_date_time,
                "‡∏Ç‡∏ô‡∏≤‡∏îBandwidth (‡∏´‡∏ô‡πà‡∏ß‡∏¢ Mbps)": bandwidth_to_use, 
                "In_Averagebps": "0",
                "Out_Averagebps": "0",
                "_raw_incoming": 0, # ‡πÄ‡∏Å‡πá‡∏ö‡∏Ñ‡πà‡∏≤‡∏î‡∏¥‡∏ö‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢
                "_raw_outcoming": 0 # ‡πÄ‡∏Å‡πá‡∏ö‡∏Ñ‡πà‡∏≤‡∏î‡∏¥‡∏ö‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢
            }
        current_date += datetime.timedelta(days=1) # ‡πÄ‡∏•‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏õ‡∏¢‡∏±‡∏á‡∏ß‡∏±‡∏ô‡∏ñ‡∏±‡∏î‡πÑ‡∏õ

    # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å API ‡∏ó‡∏±‡∏ö‡∏•‡∏á‡πÉ‡∏ô‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ß‡πâ
    for item in data_to_process:
        dt_obj_from_api = None
        if isinstance(item.get('Timestamp'), dict) and 'date' in item['Timestamp']:
            try:
                dt_obj_from_api = datetime.datetime.strptime(item['Timestamp']['date'], '%Y-%m-%d %H:%M:%S.%f')
            except ValueError:
                continue
        
        if dt_obj_from_api:
            date_key = dt_obj_from_api.strftime('%Y-%m-%d')
            formatted_time_from_api = dt_obj_from_api.strftime('%Y-%m-%d %H.%M.%S')
            
            if date_key in full_data_structure and formatted_time_from_api in full_data_structure[date_key]:
                in_avg_bps = item.get('In_Averagebps', '0')
                out_avg_bps = item.get('Out_Averagebps', '0')

                # ‡πÅ‡∏õ‡∏•‡∏á‡πÅ‡∏•‡∏∞‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏öIn_Averagebps
                try:
                    in_avg_bps_float = float(in_avg_bps)
                    full_data_structure[date_key][formatted_time_from_api]["_raw_incoming"] = int(in_avg_bps_float) 
                    full_data_structure[date_key][formatted_time_from_api]["In_Averagebps"] = f"{int(in_avg_bps_float):,}"
                except (ValueError, TypeError):
                    full_data_structure[date_key][formatted_time_from_api]["_raw_incoming"] = 0
                    full_data_structure[date_key][formatted_time_from_api]["In_Averagebps)"] = str(in_avg_bps)

                # ‡πÅ‡∏õ‡∏•‡∏á‡πÅ‡∏•‡∏∞‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏öOut_Averagebps
                try:
                    out_avg_bps_float = float(out_avg_bps)
                    full_data_structure[date_key][formatted_time_from_api]["_raw_outcoming"] = int(out_avg_bps_float)
                    full_data_structure[date_key][formatted_time_from_api]["Out_Averagebps"] = f"{int(out_avg_bps_float):,}"
                except (ValueError, TypeError):
                    full_data_structure[date_key][formatted_time_from_api]["_raw_outcoming"] = 0
                    full_data_structure[date_key][formatted_time_from_api]["Out_Averagebps"] = str(out_avg_bps)
                
                # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï "‡∏£‡∏´‡∏±‡∏™‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô", "‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô" ‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡πà‡∏≤‡∏à‡∏≤‡∏Å API ‡∏´‡∏≤‡∏Å‡∏°‡∏µ
                full_data_structure[date_key][formatted_time_from_api]["‡∏£‡∏´‡∏±‡∏™‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô"] = item.get(column_mapping["‡∏£‡∏´‡∏±‡∏™‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô"], api_customer_circuit_id)
                full_data_structure[date_key][formatted_time_from_api]["‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô"] = item.get(column_mapping["‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô"], address_to_use)
                
                # ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• Bandwidth ‡∏à‡∏≤‡∏Å item ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á ‡∏´‡∏≤‡∏Å‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡∏à‡∏≤‡∏Å API ‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
                item_bandwidth_raw = item.get(column_mapping["‡∏Ç‡∏ô‡∏≤‡∏îBandwidth (‡∏´‡∏ô‡πà‡∏ß‡∏¢ Mbps)"], '')
                if item_bandwidth_raw:
                    if "FTTx" in str(item_bandwidth_raw):
                        full_data_structure[date_key][formatted_time_from_api]["‡∏Ç‡∏ô‡∏≤‡∏îBandwidth (‡∏´‡∏ô‡πà‡∏ß‡∏¢ Mbps)"] = "20 Mbps."
                    else:
                        try:
                            numeric_value_match = re.search(r'[\d.]+', str(item_bandwidth_raw))
                            if numeric_value_match:
                                numeric_value = float(numeric_value_match.group())
                                full_data_structure[date_key][formatted_time_from_api]["‡∏Ç‡∏ô‡∏≤‡∏îBandwidth (‡∏´‡∏ô‡πà‡∏ß‡∏¢ Mbps)"] = f"{int(numeric_value)} Mbps."
                            else:
                                full_data_structure[date_key][formatted_time_from_api]["‡∏Ç‡∏ô‡∏≤‡∏îBandwidth (‡∏´‡∏ô‡πà‡∏ß‡∏¢ Mbps)"] = str(item_bandwidth_raw)
                        except (ValueError, TypeError, AttributeError):
                            full_data_structure[date_key][formatted_time_from_api]["‡∏Ç‡∏ô‡∏≤‡∏îBandwidth (‡∏´‡∏ô‡πà‡∏ß‡∏¢ Mbps)"] = str(item_bandwidth_raw)
                else:
                    # ‡∏´‡∏≤‡∏Å API ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡πà‡∏≤ bandwidth ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö item ‡∏ô‡∏µ‡πâ ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÑ‡∏ß‡πâ‡∏ï‡∏≠‡∏ô‡∏ï‡πâ‡∏ô
                    full_data_structure[date_key][formatted_time_from_api]["‡∏Ç‡∏ô‡∏≤‡∏îBandwidth (‡∏´‡∏ô‡πà‡∏ß‡∏¢ Mbps)"] = bandwidth_to_use

    # ‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏•‡πâ‡∏ß‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Grand Total
    processed_data = []
    total_incoming_sum = 0
    total_outcoming_sum = 0
    data_points_count = 0

    for date_key in sorted(full_data_structure.keys()):
        for time_key in sorted(full_data_structure[date_key].keys()):
            row = full_data_structure[date_key][time_key]
            processed_data.append(row)
            
            total_incoming_sum += row.get("_raw_incoming", 0)
            total_outcoming_sum += row.get("_raw_outcoming", 0)
            data_points_count += 1

    average_incoming = 0
    average_outcoming = 0
    if data_points_count > 0:
        average_incoming = round(total_incoming_sum / data_points_count)
        average_outcoming = round(total_outcoming_sum / data_points_count)

    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏ñ‡∏ß Grand Total (Average)
    grand_total_row = {
        "‡∏£‡∏´‡∏±‡∏™‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô": "Grand Total", 
        "‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô": "", 
        "‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏•‡∏∞‡πÄ‡∏ß‡∏•‡∏≤": "", 
        "‡∏Ç‡∏ô‡∏≤‡∏îBandwidth (‡∏´‡∏ô‡πà‡∏ß‡∏¢ Mbps)": "", 
        "In_Averagebps": f"{average_incoming:,}",
        "Out_Averagebps": f"{average_outcoming:,}"
    }

    return desired_headers_th, processed_data, grand_total_row

def export_to_csv(headers, data, filename, job_id, node_name):
    """‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå CSV"""
    try:
        with open(filename, 'w', newline='', encoding='utf-8-sig') as f:
            cw = csv.writer(f)
            if headers and data:
                cw.writerow(headers)
                last_customer_id = None
                last_customer_name = None
                for row in data:
                    new_row = [
                        row.get('‡∏£‡∏´‡∏±‡∏™‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô', ''),
                        row.get('‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô', ''),
                        row.get('‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏•‡∏∞‡πÄ‡∏ß‡∏•‡∏≤', ''),
                        row.get('‡∏Ç‡∏ô‡∏≤‡∏îBandwidth (‡∏´‡∏ô‡πà‡∏ß‡∏¢ Mbps)', ''),
                        row.get('In_Averagebps', ''),
                        row.get('Out_Averagebps', '')
                    ]
                    
                    cw.writerow(new_row)
            else:
                cw.writerow(["No Data"])
        logger.info(f"‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á CSV ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö '{node_name}' ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß")
        return True, "Success"
    except Exception as e:
        logger.error(f"‚ùå ‡∏™‡∏£‡πâ‡∏≤‡∏á CSV ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö '{node_name}' ‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß: {e}")
        return False, str(e)

def export_to_pdf(headers, daily_data, grand_total_row, filename, job_id, node_name):
    """
    ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå PDF ‡πÇ‡∏î‡∏¢‡πÉ‡∏´‡πâ‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ß‡∏±‡∏ô‡∏Ç‡∏∂‡πâ‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡πÉ‡∏´‡∏°‡πà, Grand Total ‡∏≠‡∏¢‡∏π‡πà‡∏ï‡πà‡∏≠‡∏ó‡πâ‡∏≤‡∏¢‡∏ß‡∏±‡∏ô‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢
    ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• "‡∏£‡∏´‡∏±‡∏™‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô" ‡πÅ‡∏•‡∏∞ "‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô" ‡∏à‡∏∞‡πÅ‡∏™‡∏î‡∏á‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏ï‡πà‡∏≠‡∏ß‡∏±‡∏ô
    """
    try:
        doc = SimpleDocTemplate(filename, pagesize=letter) 
        styles = getSampleStyleSheet()
        elements = []

        if headers and daily_data:
            data_by_date = {}
            # ‡∏à‡∏±‡∏î‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏≤‡∏°‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà
            for row in daily_data:
                date_time_str = row.get('‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏•‡∏∞‡πÄ‡∏ß‡∏•‡∏≤', '')
                try:
                    date_key = datetime.datetime.strptime(date_time_str, '%Y-%m-%d %H.%M.%S').strftime('%Y-%m-%d')
                except ValueError:
                    date_key = 'Uncategorized'
                    logger.warning(f"Job {job_id}: Found uncategorized date for PDF: {date_time_str}")
                if date_key not in data_by_date:
                    data_by_date[date_key] = []
                data_by_date[date_key].append(row)
            
            # --- ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö "‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏õ‡∏£‡∏∞‡∏à‡∏≥‡πÄ‡∏î‡∏∑‡∏≠‡∏ô" ‡πÇ‡∏î‡∏¢‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ---
            report_month_str = "‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏‡πÄ‡∏î‡∏∑‡∏≠‡∏ô"
            if data_by_date:
                first_date_str = sorted(data_by_date.keys())[0]
                try:
                    first_date_obj = datetime.datetime.strptime(first_date_str, '%Y-%m-%d')
                    thai_months = {
                        1: "‡∏°‡∏Å‡∏£‡∏≤‡∏Ñ‡∏°", 2: "‡∏Å‡∏∏‡∏°‡∏†‡∏≤‡∏û‡∏±‡∏ô‡∏ò‡πå", 3: "‡∏°‡∏µ‡∏ô‡∏≤‡∏Ñ‡∏°", 4: "‡πÄ‡∏°‡∏©‡∏≤‡∏¢‡∏ô",
                        5: "‡∏û‡∏§‡∏©‡∏†‡∏≤‡∏Ñ‡∏°", 6: "‡∏°‡∏¥‡∏ñ‡∏∏‡∏ô‡∏≤‡∏¢‡∏ô", 7: "‡∏Å‡∏£‡∏Å‡∏é‡∏≤‡∏Ñ‡∏°", 8: "‡∏™‡∏¥‡∏á‡∏´‡∏≤‡∏Ñ‡∏°",
                        9: "‡∏Å‡∏±‡∏ô‡∏¢‡∏≤‡∏¢‡∏ô", 10: "‡∏ï‡∏∏‡∏•‡∏≤‡∏Ñ‡∏°", 11: "‡∏û‡∏§‡∏®‡∏à‡∏¥‡∏Å‡∏≤‡∏¢‡∏ô", 12: "‡∏ò‡∏±‡∏ô‡∏ß‡∏≤‡∏Ñ‡∏°"
                    }
                    report_month_str = thai_months.get(first_date_obj.month, "‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏‡πÄ‡∏î‡∏∑‡∏≠‡∏ô")
                except ValueError:
                    logger.warning(f"Job {job_id}: Could not parse first date for month determination: {first_date_str}")

            first_page = True
            sorted_dates = sorted(data_by_date.keys()) 
            
            for i, date_key in enumerate(sorted_dates): # ‡∏ß‡∏ô‡∏•‡∏π‡∏õ‡∏ï‡∏≤‡∏°‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏ó‡∏µ‡πà‡∏à‡∏±‡∏î‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡πÅ‡∏•‡πâ‡∏ß
                group_data = data_by_date[date_key]

                if not first_page:
                    elements.append(PageBreak()) # ‡∏Ç‡∏∂‡πâ‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡πÉ‡∏´‡∏°‡πà‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ß‡∏±‡∏ô
                
                # ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏™‡πÑ‡∏ï‡∏•‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏´‡∏±‡∏ß‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á
                title_style = styles['Title']
                if THAI_FONT_REGISTERED:
                    title_style.fontName = THAI_FONT_NAME
                title_style.fontSize = 18
                title_style.alignment = 1 # ‡∏à‡∏±‡∏î‡∏Å‡∏∂‡πà‡∏á‡∏Å‡∏•‡∏≤‡∏á
                elements.append(Paragraph("Customer Interface Summary Report by Hour", title_style))
                elements.append(Spacer(1, 0.2 * inch))

                # ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏™‡πÑ‡∏ï‡∏•‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏õ‡∏£‡∏∞‡∏à‡∏≥‡πÄ‡∏î‡∏∑‡∏≠‡∏ô
                month_report_style = ParagraphStyle('MonthReport', parent=styles['Normal'])
                if THAI_FONT_REGISTERED:
                    month_report_style.fontName = THAI_FONT_NAME
                month_report_style.fontSize = 14
                month_report_style.alignment = 1
                elements.append(Paragraph(f"‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏õ‡∏£‡∏∞‡∏à‡∏≥‡πÄ‡∏î‡∏∑‡∏≠‡∏ô {report_month_str}", month_report_style))
                elements.append(Spacer(1, 0.2 * inch))

                # ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏™‡πÑ‡∏ï‡∏•‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà
                date_header_style = ParagraphStyle('DateHeader', parent=styles['Normal'])
                if THAI_FONT_REGISTERED:
                    date_header_style.fontName = THAI_FONT_NAME
                date_header_style.fontSize = 12
                date_header_style.alignment = 1
                elements.append(Paragraph(f"<b>‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà </b> {date_key}", date_header_style))
                elements.append(Spacer(1, 0.2 * inch))

                # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏´‡∏±‡∏ß‡∏ï‡∏≤‡∏£‡∏≤‡∏á
                table_headers = [
                    "‡∏£‡∏´‡∏±‡∏™‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô",
                    "‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô",
                    "‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏•‡∏∞‡πÄ‡∏ß‡∏•‡∏≤",
                    "‡∏Ç‡∏ô‡∏≤‡∏îBandwidth (‡∏´‡∏ô‡πà‡∏ß‡∏¢ Mbps)",
                    "‡∏õ‡∏£‡∏¥‡∏°‡∏≤‡∏ì‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô incoming (‡∏´‡∏ô‡πà‡∏ß‡∏¢ bps)",
                    "‡∏õ‡∏£‡∏¥‡∏°‡∏≤‡∏ì‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô outcoming (‡∏´‡∏ô‡πà‡∏ß‡∏¢ bps)"
                ]
                
                table_data = [table_headers] # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏´‡∏±‡∏ß‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏≤‡∏£‡∏≤‡∏á

                last_customer_id = None
                last_customer_name = None
                
                for idx_row, row in enumerate(group_data):
                    current_customer_id = row.get('‡∏£‡∏´‡∏±‡∏™‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô', '')
                    current_customer_name = row.get('‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô', '')
                    current_bandwidth = row.get('‡∏Ç‡∏ô‡∏≤‡∏îBandwidth (‡∏´‡∏ô‡πà‡∏ß‡∏¢ Mbps)', '')

                    display_customer_id = current_customer_id
                    display_customer_name = current_customer_name
                    display_bandwidth = current_bandwidth # Bandwidth ‡∏à‡∏∞‡πÅ‡∏™‡∏î‡∏á‡∏ó‡∏∏‡∏Å‡πÅ‡∏ñ‡∏ß

                    # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡πÅ‡∏ñ‡∏ß‡πÅ‡∏£‡∏Å‡∏Ç‡∏≠‡∏á‡∏Å‡∏•‡∏∏‡πà‡∏° ‡πÅ‡∏•‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ö‡πÅ‡∏ñ‡∏ß‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤ ‡πÉ‡∏´‡πâ‡πÄ‡∏ß‡πâ‡∏ô‡∏ß‡πà‡∏≤‡∏á
                    if idx_row > 0:
                        if current_customer_id == last_customer_id:
                            display_customer_id = ''
                        if current_customer_name == last_customer_name:
                            display_customer_name = ''

                    table_data.append([
                        display_customer_id,
                        display_customer_name,
                        row.get('‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏•‡∏∞‡πÄ‡∏ß‡∏•‡∏≤', ''),
                        display_bandwidth, # ‡πÅ‡∏™‡∏î‡∏á bandwidth ‡πÄ‡∏™‡∏°‡∏≠
                        row.get('In_Averagebps', ''),
                        row.get('Out_Averagebps', '')
                    ])
                    
                    # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏Ñ‡πà‡∏≤ last_ ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ß‡∏ô‡∏ã‡πâ‡∏≥‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ñ‡∏±‡∏î‡πÑ‡∏õ
                    last_customer_id = current_customer_id
                    last_customer_name = current_customer_name
                
                # ‡∏´‡∏≤‡∏Å‡πÄ‡∏õ‡πá‡∏ô‡∏ß‡∏±‡∏ô‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢ ‡πÅ‡∏•‡∏∞‡∏°‡∏µ‡πÅ‡∏ñ‡∏ß Grand Total ‡πÉ‡∏´‡πâ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô‡∏ï‡∏≤‡∏£‡∏≤‡∏á
                if i == len(sorted_dates) - 1 and grand_total_row:
                    table_data.append([
                        grand_total_row.get('‡∏£‡∏´‡∏±‡∏™‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô', ''),
                        grand_total_row.get('‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô', ''),
                        grand_total_row.get('‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏•‡∏∞‡πÄ‡∏ß‡∏•‡∏≤', ''),
                        grand_total_row.get('‡∏Ç‡∏ô‡∏≤‡∏îBandwidth (‡∏´‡∏ô‡πà‡∏ß‡∏¢ Mbps)', ''),
                        grand_total_row.get('In_Averagebps', ''),
                        grand_total_row.get('Out_Averagebps', '')
                    ])

                table = Table(table_data)
                # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏™‡πÑ‡∏ï‡∏•‡πå‡∏Ç‡∏≠‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á
                table_style = [
                    ('BACKGROUND', (0, 0), (-1, 0), '#cccccc'), # ‡∏û‡∏∑‡πâ‡∏ô‡∏´‡∏•‡∏±‡∏á‡∏´‡∏±‡∏ß‡∏ï‡∏≤‡∏£‡∏≤‡∏á
                    ('TEXTCOLOR', (0, 0), (-1, 0), '#000000'), # ‡∏™‡∏µ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡∏´‡∏±‡∏ß‡∏ï‡∏≤‡∏£‡∏≤‡∏á
                    ('ALIGN', (0, 0), (-1, -1), 'CENTER'), # ‡∏à‡∏±‡∏î‡∏Å‡∏∂‡πà‡∏á‡∏Å‡∏•‡∏≤‡∏á‡∏ó‡∏∏‡∏Å‡πÄ‡∏ã‡∏•‡∏•‡πå
                    ('BOTTOMPADDING', (0, 0), (-1, 0), 12), # ‡∏£‡∏∞‡∏¢‡∏∞‡∏´‡πà‡∏≤‡∏á‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡∏Ç‡∏≠‡∏á‡∏´‡∏±‡∏ß‡∏ï‡∏≤‡∏£‡∏≤‡∏á
                    ('BACKGROUND', (0, 1), (-1, -1), "#ffffff"), # ‡∏û‡∏∑‡πâ‡∏ô‡∏´‡∏•‡∏±‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏≤‡∏£‡∏≤‡∏á
                    ('GRID', (0, 0), (-1, -1), 1, '#999999'), # ‡πÄ‡∏™‡πâ‡∏ô‡∏Å‡∏£‡∏¥‡∏î
                    ('FONTSIZE', (0, 0), (-1, -1), 10), # ‡∏Ç‡∏ô‡∏≤‡∏î‡∏ü‡∏≠‡∏ô‡∏ï‡πå
                    ('LEFTPADDING', (0,0), (-1,-1), 6), # ‡∏£‡∏∞‡∏¢‡∏∞‡∏´‡πà‡∏≤‡∏á‡∏ã‡πâ‡∏≤‡∏¢
                    ('RIGHTPADDING', (0,0), (-1,-1), 6), # ‡∏£‡∏∞‡∏¢‡∏∞‡∏´‡πà‡∏≤‡∏á‡∏Ç‡∏ß‡∏≤
                ]
                
                # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ü‡∏≠‡∏ô‡∏ï‡πå‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏´‡∏≤‡∏Å‡∏•‡∏á‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡πÑ‡∏ß‡πâ
                if THAI_FONT_REGISTERED:
                    table_style.append(('FONTNAME', (0, 0), (-1, 0), THAI_FONT_NAME)) # ‡∏ü‡∏≠‡∏ô‡∏ï‡πå‡∏´‡∏±‡∏ß‡∏ï‡∏≤‡∏£‡∏≤‡∏á
                    table_style.append(('FONTNAME', (0, 1), (-1, -1), THAI_FONT_NAME)) # ‡∏ü‡∏≠‡∏ô‡∏ï‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏≤‡∏£‡∏≤‡∏á
                else:
                    table_style.append(('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'))
                    table_style.append(('FONTNAME', (0, 1), (-1, -1), 'Helvetica'))
                
                # ‡∏™‡πÑ‡∏ï‡∏•‡πå‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ñ‡∏ß Grand Total
                if i == len(sorted_dates) - 1 and grand_total_row:
                    grand_total_row_index = len(table_data) - 1
                    table_style.append(('BACKGROUND', (0, grand_total_row_index), (-1, grand_total_row_index), '#dddddd')) # ‡∏û‡∏∑‡πâ‡∏ô‡∏´‡∏•‡∏±‡∏á
                    table_style.append(('FONTNAME', (0, grand_total_row_index), (-1, grand_total_row_index), THAI_FONT_NAME if THAI_FONT_REGISTERED else 'Helvetica-Bold')) # ‡∏ü‡∏≠‡∏ô‡∏ï‡πå
                    table_style.append(('SPAN', (0, grand_total_row_index), (3, grand_total_row_index))) # ‡∏£‡∏ß‡∏°‡πÄ‡∏ã‡∏•‡∏•‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö "Grand Total"
                    table_style.append(('ALIGN', (0, grand_total_row_index), (3, grand_total_row_index), 'LEFT')) # ‡∏à‡∏±‡∏î‡∏ä‡∏¥‡∏î‡∏ã‡πâ‡∏≤‡∏¢
                    table_style.append(('VALIGN', (0, grand_total_row_index), (-1, grand_total_row_index), 'MIDDLE')) # ‡∏à‡∏±‡∏î‡πÅ‡∏ô‡∏ß‡∏ï‡∏±‡πâ‡∏á‡∏Å‡∏∂‡πà‡∏á‡∏Å‡∏•‡∏≤‡∏á

                table.setStyle(table_style)
                elements.append(table)
                elements.append(Spacer(1, 0.5 * inch))

                first_page = False # ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô False ‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏£‡∏Å
            
        else:
            # ‡∏Å‡∏£‡∏ì‡∏µ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
            no_data_style = styles['Normal']
            if THAI_FONT_REGISTERED:
                no_data_style.fontName = THAI_FONT_NAME
            elements.append(Paragraph("No circuit status data available.", no_data_style))
        
        doc.build(elements) # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ PDF
        logger.info(f"‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á PDF ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö '{node_name}' ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß")
        return True, "PDF generated successfully."
    except Exception as e:
        logger.error(f"‚ùå ‡∏™‡∏£‡πâ‡∏≤‡∏á PDF ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö '{node_name}' ‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß: {e}")
        return False, f"Error generating PDF: {e}"

def process_file_in_background(file_stream, job_id):
    """
    ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÉ‡∏ô‡∏≠‡∏µ‡∏Å Thread ‡∏´‡∏ô‡∏∂‡πà‡∏á
    ‡πÇ‡∏î‡∏¢‡∏à‡∏∞‡∏£‡∏±‡∏ö file_stream (‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏ü‡∏•‡πå) ‡πÅ‡∏•‡∏∞ job_id ‡∏°‡∏≤‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•
    """
    temp_dir = None
    try:
        df = pd.read_excel(file_stream)
        total_rows = len(df)
        with status_lock:
            processing_status[job_id]['total'] = total_rows
            processing_status[job_id]['results'] = []
            temp_dir = tempfile.mkdtemp(prefix=f"report_job_{job_id}_")
            processing_status[job_id]['temp_dir'] = temp_dir
        
        logger.info(f"üìä ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå Excel ‡∏°‡∏µ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î {total_rows} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")

        required_columns = ['NodeID', 'Interface ID', '‡∏Å‡∏£‡∏∞‡∏ó‡∏£‡∏ß‡∏á / ‡∏™‡∏±‡∏á‡∏Å‡∏±‡∏î', '‡∏Å‡∏£‡∏° / ‡∏™‡∏±‡∏á‡∏Å‡∏±‡∏î', '‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î', '‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô', 'Node Name']
        if not all(col in df.columns for col in required_columns):
            missing_cols = [c for c in required_columns if c not in df.columns]
            with status_lock:
                processing_status[job_id]['error'] = f"‡πÑ‡∏ü‡∏•‡πå Excel ‡∏Ç‡∏≤‡∏î‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô: {', '.join(missing_cols)}"
                processing_status[job_id]['completed'] = True
            logger.error(f"‚ùå {processing_status[job_id]['error']}")
            return
        
        csv_root_dir = os.path.join(temp_dir, 'CSV')
        pdf_root_dir = os.path.join(temp_dir, 'PDF')
        os.makedirs(csv_root_dir, exist_ok=True)
        os.makedirs(pdf_root_dir, exist_ok=True)
        
        for index, row in df.iterrows():
            with status_lock:
                if processing_status[job_id].get('canceled'):
                    logger.info(f"‚õî ‡∏á‡∏≤‡∏ô‡∏ñ‡∏π‡∏Å‡∏¢‡∏Å‡πÄ‡∏•‡∏¥‡∏Å‡πÇ‡∏î‡∏¢‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ")
                    break
            
            node_name = ''
            csv_success = False
            pdf_success = False
            error_message = None

            try:
                nod_id = str(row['NodeID']).strip()
                itf_id = str(row['Interface ID']).strip()
                
                folder1 = str(row['‡∏Å‡∏£‡∏∞‡∏ó‡∏£‡∏ß‡∏á / ‡∏™‡∏±‡∏á‡∏Å‡∏±‡∏î']).strip()
                folder2 = str(row['‡∏Å‡∏£‡∏° / ‡∏™‡∏±‡∏á‡∏Å‡∏±‡∏î']).strip()
                folder3 = str(row['‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î']).strip()
                folder4 = str(row['‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô']).strip()
                node_name = str(row['Node Name']).strip()

                if not nod_id or not itf_id:
                    error_message = "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• NodeID ‡∏´‡∏£‡∏∑‡∏≠ Interface ID ‡πÑ‡∏°‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå"
                    logger.warning(f"‚ö†Ô∏è ‡∏Ç‡πâ‡∏≤‡∏°‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà {index + 1} ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å {error_message} (NodeID: '{nod_id}', ITF ID: '{itf_id}')")
                    with status_lock:
                        processing_status[job_id]['processed'] += 1
                        processing_status[job_id]['results'].append({
                            'node_name': node_name,
                            'csv_success': False,
                            'pdf_success': False,
                            'error_message': error_message
                        })
                    continue
                
                logger.info(f"‚ñ∂ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• NodeID: {nod_id}, Interface ID: {itf_id} (‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà {index + 1})")

                current_csv_dir = os.path.join(csv_root_dir, folder1, folder2, folder3, folder4)
                current_pdf_dir = os.path.join(pdf_root_dir, folder1, folder2, folder3, folder4)
                
                os.makedirs(current_csv_dir, exist_ok=True)
                os.makedirs(current_pdf_dir, exist_ok=True)
                
                raw_json_data = get_data_from_api(nod_id, itf_id, job_id)

                if raw_json_data:
                    headers, processed_daily_data, grand_total_row_data = process_json_data(raw_json_data, job_id, nod_id, folder4)
                    
                    sanitized_node_name = re.sub(r'[\\/:*?"<>|]', '_', node_name)
                    filename_base = f"{sanitized_node_name}" 

                    csv_filename = os.path.join(current_csv_dir, f"{filename_base}.csv")
                    pdf_filename = os.path.join(current_pdf_dir, f"{filename_base}.pdf")

                    # For CSV, append grand_total_row_data to processed_daily_data
                    csv_data_to_write = list(processed_daily_data) # Create a copy
                    if grand_total_row_data:
                        csv_data_to_write.append(grand_total_row_data)

                    csv_success, csv_msg = export_to_csv(headers, csv_data_to_write, csv_filename, job_id, node_name)
                    pdf_success, pdf_msg = export_to_pdf(headers, processed_daily_data, grand_total_row_data, pdf_filename, job_id, node_name)
                else:
                    error_message = f"‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å API ‡πÑ‡∏î‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö NodeID: {nod_id}, Interface ID: {itf_id}"
                    logger.error(f"‚ùå {error_message}")
            
            except Exception as e:
                error_message = f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏Ñ‡∏≤‡∏î‡∏Ñ‡∏¥‡∏î‡πÉ‡∏ô‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà {index + 1}: {e}"
                logger.error(f"‚ùå {error_message}")
                
            finally:
                with status_lock:
                    processing_status[job_id]['processed'] += 1
                    processing_status[job_id]['results'].append({
                        'node_name': node_name,
                        'csv_success': csv_success,
                        'pdf_success': pdf_success,
                        'error_message': error_message
                    })
        
        if not processing_status[job_id].get('canceled'):
            zip_filename = f"customer_reports_{job_id}_{datetime.datetime.now().strftime('%Y%m%d%H%M%S')}.zip"
            zip_file_path = os.path.join(tempfile.gettempdir(), zip_filename)
            
            if temp_dir and os.path.exists(temp_dir):
                with zipfile.ZipFile(zip_file_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
                    for root, _, files in os.walk(temp_dir):
                        for file in files:
                            file_path = os.path.join(root, file)
                            arcname = os.path.relpath(file_path, temp_dir)
                            zipf.write(file_path, arcname)
                
                with status_lock:
                    processing_status[job_id]['zip_file_path'] = zip_file_path
                    processing_status[job_id]['completed'] = True
                logger.info(f"‚úÖ ‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå! ‡πÑ‡∏ü‡∏•‡πå ZIP: {zip_file_path.split(os.sep)[-1]}")
            else:
                with status_lock:
                    processing_status[job_id]['error'] = "‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡∏£‡πâ‡∏≤‡∏á ZIP"
                    processing_status[job_id]['completed'] = True
                logger.error(f"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß '{temp_dir}' ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á ZIP ‡πÑ‡∏î‡πâ")
        else:
            with status_lock:
                 processing_status[job_id]['completed'] = True
                 processing_status[job_id]['error'] = "‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏ñ‡∏π‡∏Å‡∏¢‡∏Å‡πÄ‡∏•‡∏¥‡∏Å"

    except Exception as e:
        with status_lock:
            processing_status[job_id]['error'] = f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏´‡∏•‡∏±‡∏á: {e}"
            processing_status[job_id]['completed'] = True
        logger.critical(f"‚ùå {processing_status[job_id]['error']}")
    finally:
        if temp_dir and os.path.exists(temp_dir):
            try:
                shutil.rmtree(temp_dir, ignore_errors=True)
                # ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡∏Å‡∏£‡∏≠‡∏á‡πÇ‡∏î‡∏¢ QueueHandler ‡πÅ‡∏•‡πâ‡∏ß
                logger.info(f"üìÅ ‡∏•‡∏ö‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå CSV/PDF ‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß: {temp_dir.split(os.sep)[-1]} ‡πÅ‡∏•‡πâ‡∏ß (‡πÑ‡∏°‡πà‡∏£‡∏ß‡∏°‡πÑ‡∏ü‡∏•‡πå ZIP)") 
            except Exception as e:
                logger.error(f"‚ùå ‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏•‡∏ö‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå CSV/PDF ‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß: {e}")

@app.route('/')
def upload_form():
    """‡πÅ‡∏™‡∏î‡∏á‡∏´‡∏ô‡πâ‡∏≤‡∏ü‡∏≠‡∏£‡πå‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå Excel"""
    return render_template('index.html')

@app.route('/generate_report', methods=['POST'])
def generate_report():
    """
    ‡∏£‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î ‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÉ‡∏ô‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏´‡∏•‡∏±‡∏á
    """
    if 'excel_file' not in request.files:
        return jsonify({"error": "No file part"}), 400
    
    file = request.files['excel_file']
    if file.filename == '':
        return jsonify({"error": "No selected file"}), 400
    
    if file:
        job_id = str(uuid.uuid4())
        file_stream = io.BytesIO(file.read())
        
        with status_lock:
            processing_status[job_id] = {
                'total': -1, 
                'processed': 0, 
                'completed': False, 
                'error': None,
                'canceled': False,
                'results': [],
                'temp_dir': None,
                'zip_file_path': None,
                'timestamp': datetime.datetime.now()
            }
        logger.info(f"üìÇ ‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå excel '{file.filename}' ‡πÅ‡∏•‡∏∞‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• (Job ID: {job_id})")

        thread = threading.Thread(target=process_file_in_background, args=(file_stream, job_id))
        thread.daemon = True
        thread.start()
        
        return jsonify({"message": "Processing started", "job_id": job_id})

@app.route('/status/<job_id>')
def get_status(job_id):
    """
    ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Ç‡∏≠‡∏á‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏≠‡∏¢‡∏π‡πà
    """
    with status_lock:
        status = processing_status.get(job_id, {})
    return jsonify(status)

@app.route('/logs/<job_id>')
def get_logs(job_id):
    """
    ‡∏î‡∏∂‡∏á log ‡∏Ç‡∏≠‡∏á‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏≠‡∏¢‡∏π‡πà
    """
    logs = []
    while not log_queue.empty():
        try:
            logs.append(log_queue.get_nowait())
        except Exception:
            break
    return jsonify({"logs": logs})


@app.route('/cancel/<job_id>', methods=['POST'])
def cancel_job(job_id):
    """
    ‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏¢‡∏Å‡πÄ‡∏•‡∏¥‡∏Å‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏≠‡∏¢‡∏π‡πà
    """
    with status_lock:
        if job_id in processing_status:
            processing_status[job_id]['canceled'] = True
            logger.info(f"‚õî ‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏Ç‡∏≠‡∏¢‡∏Å‡πÄ‡∏•‡∏¥‡∏Å‡∏á‡∏≤‡∏ô (Job ID: {job_id})")
            return jsonify({"message": "Job cancellation requested"}), 200
        else:
            logger.warning(f"‚ö†Ô∏è ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏¢‡∏Å‡πÄ‡∏•‡∏¥‡∏Å‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏û‡∏ö (Job ID: {job_id})")
            return jsonify({"error": "Job not found"}), 404

@app.route('/download_report/<job_id>')
def download_report(job_id):
    """
    ‡πÉ‡∏´‡πâ‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå ZIP ‡∏ó‡∏µ‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡∏∂‡πâ‡∏ô
    """
    with status_lock:
        job_info = processing_status.get(job_id)

    if not job_info:
        logger.error(f"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏á‡∏≤‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î (Job ID: {job_id})")
        return jsonify({"error": "Job not found or not ready for download. It might be too old or cancelled."}), 404

    zip_file_path = job_info.get('zip_file_path')

    if not zip_file_path or not os.path.exists(zip_file_path):
        logger.error(f"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå ZIP ‡∏´‡∏£‡∏∑‡∏≠‡∏¢‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏°‡πà‡πÄ‡∏™‡∏£‡πá‡∏à (Job ID: {job_id}). Path: {zip_file_path}")
        if job_info.get('completed') and not zip_file_path:
            return jsonify({"error": "Report completed with no ZIP file generated (internal error)"}), 500
        return jsonify({"error": "Report not yet generated or file not found"}), 404
    
    try:
        directory = tempfile.gettempdir()
        filename = os.path.basename(zip_file_path)
        logger.info(f"üì• ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡πà‡∏á‡πÑ‡∏ü‡∏•‡πå ZIP: {filename} ‡∏à‡∏≤‡∏Å {directory} (Job ID: {job_id})")
        
        response = send_from_directory(
            directory=directory,
            path=filename,
            as_attachment=True,
            mimetype='application/zip',
            download_name="Customer Report by Hour.zip"
        )
        
        return response

    except Exception as e:
        logger.critical(f"‚ùå ‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏£‡πâ‡∏≤‡∏¢‡πÅ‡∏£‡∏á‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡πà‡∏á‡πÑ‡∏ü‡∏•‡πå ZIP: {e} (Job ID: {job_id})")
        return jsonify({"error": f"Failed to serve file: {e}"}), 500

def cleanup_old_jobs():
    """
    ‡∏•‡∏ö‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏á‡∏≤‡∏ô‡πÅ‡∏•‡∏∞‡πÑ‡∏ü‡∏•‡πå ZIP ‡πÄ‡∏Å‡πà‡∏≤‡πÜ ‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å‡∏£‡∏∞‡∏ö‡∏ö
    ‡∏£‡∏±‡∏ô‡πÄ‡∏õ‡πá‡∏ô background process
    """
    logger.info("üßπ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏•‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏á‡∏≤‡∏ô‡πÄ‡∏Å‡πà‡∏≤...")
    current_time = datetime.datetime.now()
    jobs_to_remove = []

    retention_hours = 24
    retention_seconds = retention_hours * 3600

    with status_lock:
        for job_id, job_info in list(processing_status.items()):
            if job_info.get('completed') and job_info.get('timestamp'): 
                job_timestamp = job_info['timestamp']
                if (current_time - job_timestamp).total_seconds() > retention_seconds:
                    jobs_to_remove.append(job_id)
            elif (not job_info.get('completed')) and (current_time - job_info.get('timestamp', current_time)).total_seconds() > (retention_seconds / 4):
                logger.warning(f"‚ö†Ô∏è ‡∏û‡∏ö‡∏á‡∏≤‡∏ô‡∏Ñ‡πâ‡∏≤‡∏á‡πÄ‡∏Å‡πà‡∏≤ (‡πÑ‡∏°‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå) ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ñ‡∏π‡∏Å‡∏•‡∏ö: {job_id}")
                jobs_to_remove.append(job_id)


    for job_id in jobs_to_remove:
        with status_lock:
            job_info = processing_status.pop(job_id, None) 
        if job_info:
            zip_file_path = job_info.get('zip_file_path')
            if zip_file_path and os.path.exists(zip_file_path):
                try:
                    os.remove(zip_file_path)
                    logger.info(f"üóëÔ∏è ‡∏•‡∏ö‡πÑ‡∏ü‡∏•‡πå ZIP ‡πÄ‡∏Å‡πà‡∏≤: {os.path.basename(zip_file_path)} (Job ID: {job_id})")
                except Exception as e:
                    logger.error(f"‚ùå ‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏•‡∏ö‡πÑ‡∏ü‡∏•‡πå ZIP ‡πÄ‡∏Å‡πà‡∏≤: {e} (Job ID: {job_id})")
            logger.info(f"‚ú® ‡∏•‡πâ‡∏≤‡∏á‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏á‡∏≤‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Job ID: {job_id} ‡πÅ‡∏•‡πâ‡∏ß")
    logger.info("üßπ ‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏•‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏á‡∏≤‡∏ô‡πÄ‡∏Å‡πà‡∏≤‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå")
    threading.Timer(retention_seconds / 2, cleanup_old_jobs).start()

if __name__ == '__main__':
    cleanup_thread = threading.Thread(target=cleanup_old_jobs)
    cleanup_thread.daemon = True
    cleanup_thread.start()

    app.run(debug=True)